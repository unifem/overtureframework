***************************************************************************
****** June 21, 201

set tcm3p = $op/tests/tcm3

OK: 
mpirun -np 8 $tcm3p -g=sphereInABoxe2.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

-np=16: 
PETScSolver:: ... done build matrix, cpu=8.80e+00
*** max residual=4.91e-05, time for 1st solve of the Dirichlet problem = 1.34e+01 (iterations=18) ***


-np=8:

  0 KSP Residual norm 1.469593874811e+04 
  1 KSP Residual norm 1.472487215815e+03 
  2 KSP Residual norm 3.348992446759e+02 
  3 KSP Residual norm 1.166775517819e+02 
  4 KSP Residual norm 6.481240170094e+01 
  5 KSP Residual norm 1.626359096344e+01 
  6 KSP Residual norm 7.285976334596e+00 
  7 KSP Residual norm 3.028336681741e+00 
  8 KSP Residual norm 6.033370037976e-01 
  9 KSP Residual norm 2.276687845151e-01 
 10 KSP Residual norm 8.366548331167e-02 
 11 KSP Residual norm 2.665167857718e-02 
 12 KSP Residual norm 1.035101935620e-02 
 13 KSP Residual norm 3.401610162266e-03 
 14 KSP Residual norm 1.275335077621e-03 
 15 KSP Residual norm 4.364253507439e-04 
 16 KSP Residual norm 1.441542358314e-04 
PETScSolver::Time for solve=6.52e+00 (its=16) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]

*** max residual=1.44e-04, time for 1st solve of the Dirichlet problem = 2.83e+01 (iterations=16) ***



OK: *trouble -np=2** ??
$tcm3p -g=sphereInABoxe2.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre
*** -pc_type found in the options database: [hypre]
  0 KSP Residual norm 1.504580129598e+04 
  1 KSP Residual norm 1.042437246207e+03 
  2 KSP Residual norm 1.247567022542e+02 
  3 KSP Residual norm 3.843323126131e+01 
  4 KSP Residual norm 1.284686522264e+01 
  5 KSP Residual norm 7.287464088270e+00 
  6 KSP Residual norm 3.058038323502e+00 
  7 KSP Residual norm 6.211494531145e-01 
  8 KSP Residual norm 3.124334411907e-01 
  9 KSP Residual norm 9.721372129836e-02 
 10 KSP Residual norm 4.033251740458e-02 
 11 KSP Residual norm 1.769314060239e-02 
 12 KSP Residual norm 7.324545544134e-03 
 13 KSP Residual norm 3.893201613319e-03 
 14 KSP Residual norm 9.017060151107e-04 
 15 KSP Residual norm 2.288617256085e-04 
 16 KSP Residual norm 6.814668086022e-05 
PETScSolver::Time for solve=1.71e+01 (its=16) 

OK: 
mpirun -np 4 $tcm3p -g=sphereInABoxe1.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

OK:
mpirun -np 8 $tcm3p -g=cice16.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

OK: 
$tcm3p -g=cice4.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

*** -pc_type found in the options database: [hypre]
  0 KSP Residual norm 6.547554155602e+02 
  1 KSP Residual norm 5.158686195998e+01 
  2 KSP Residual norm 4.478637225667e+00 
  3 KSP Residual norm 6.937503037368e-01 
  4 KSP Residual norm 1.363014275258e-01 
  5 KSP Residual norm 2.280934521198e-02 
  6 KSP Residual norm 3.064811032451e-03 
  7 KSP Residual norm 6.889056098088e-04 
  8 KSP Residual norm 8.505328891671e-05 
  9 KSP Residual norm 2.562108182655e-05 
 10 KSP Residual norm 3.095166775547e-06 
PETScSolver::Time for solve=1.42e-01 (its=10) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]



TEST HYPRE: works
$tcm3p -g=square64.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

  0 KSP Residual norm 1.761875507209e+02 
  1 KSP Residual norm 8.877005411439e+00 
  2 KSP Residual norm 1.937521081133e-01 
  3 KSP Residual norm 3.019956446744e-03 
  4 KSP Residual norm 5.431298376278e-05 
  5 KSP Residual norm 6.613549892860e-07 
PETScSolver::Time for solve=1.55e-02 (its=5) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]

*** max residual=6.61e-07, time for 1st solve of the Dirichlet problem = 2.84e-02 (iterations=5) ***


tcm3p -g=square20.order2 -solver=petsc -predefined


set tcmcp = $op/tests/tcmConstraintp
set tcmcs = $op/tests/tcmConstraints

TEST BOX:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=
BOX4
>>>>>>>>>  -np=1
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PS-- fill in extra eqn =0
--PS-- time to insert 1682 values for eqn 0 is  1.43e+00
--PS-- fill in extra eqn =1
--PS-- time to insert 1682 values for eqn 1 is  1.32e+00
--PES-- Time for filling user extra equations= 2.75e+00(s)
PETScSolver:: ... done build matrix, cpu=3.01e+00
PETScSolver::Time for solve=2.06e-01 (its=33) 

>>>>>>>> -np=2
-PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PS-- fill in extra eqn =0
--PS-- myid=0 time to insert 821 values for eqn 0 is  2.81e-05
--PS-- myid=1 time to insert 862 values for eqn 0 is  2.49e-01
--PS-- fill in extra eqn =1
--PS-- myid=0 time to insert 821 values for eqn 1 is  1.91e-05
--PS-- myid=1 time to insert 862 values for eqn 1 is  2.39e-01
--PES-- Time for filling user extra equations= 4.89e-01(s)
PETScSolver:: ... done build matrix, cpu=6.45e-01


>>>>>> -np=4
--PS-- fill in extra eqn =0
--PS-- myid=1 time to insert 1 values for eqn 0 is  1.26e-05
--PS-- myid=3 time to insert 1 values for eqn 0 is  9.54e-07
--PS-- myid=0 time to insert 821 values for eqn 0 is  1.98e-05
--PS-- myid=2 time to insert 862 values for eqn 0 is  1.81e-05
--PS-- fill in extra eqn =1
--PS-- myid=1 time to insert 1 values for eqn 1 is  4.77e-07
--PS-- myid=3 time to insert 1 values for eqn 1 is  4.77e-07
--PS-- myid=0 time to insert 821 values for eqn 1 is  1.03e-05
--PS-- myid=2 time to insert 862 values for eqn 1 is  6.91e-06
--PES-- Time for filling user extra equations= 7.93e-03(s)
PETScSolver:: ... done build matrix, cpu=9.06e-02





++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=


SERIAL
$tcmcs box4.order2.hdf -solver=petsc -predefined -debug=1 
+Petsc TIMINGS (for 33 its, size of matrix n = 91125 ):
 build=0.06125, precond=0.063491, solve=0.207083, total=0.331824.

Neumann: 
+Petsc TIMINGS (for 56 its, size of matrix n = 91125 ):
 build=0.098647, precond=0.948522, solve=0.2984, total=1.34557.

PARALLEL: 
mpirun -np 1 $tcmcp box4.order2.hdf -solver=petsc -predefined -debug=1 
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PES-- Time for filling user extra equations= 2.64e+00(s)
PETScSolver:: ... done build matrix, cpu=2.89e+00
PETScSolver::Time for solve=2.06e-01 (its=33) 

Neumann ***
PETScSolver:: build matrix... Oges::debug=1
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PES-- Time for filling user extra equations= 5.32e-05(s)
PETScSolver:: ... done build matrix, cpu=5.74e+01
PETScSolver::Time for solve=1.24e+00 (its=54) 



mpirun -np 1 $tcmcp box2.order2.hdf -solver=petsc -predefined -debug=1 

mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined -debug=1


***************************************************************************
****** June 3,4 2017

*** CHANGED PETSC-SOLVER TO USE OGES EQUATION NUMBERING TOO


*NEW*
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np4.out


OLD:
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out


*** CHANGE PETScSOLVER to put extra equations at the end of the matrix 


mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out




set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined -trig

Maximum relative error with dirichlet bc's= 8.996811e-03 (8.996811e-03 with ghost)
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value= 1.958 true= 2.000 err=4.24e-02
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value= 0.938 true= 1.000 err=6.25e-02

Maximum relative error with neumann bc's= 8.375221e-03
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value=11.005 true=11.000 err=4.51e-03
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value=-0.000 true= 0.000 err=6.54e-13



***************************************************************************
****** June 2, 2017


TEST TCMCONSTRAINT ON overset grid


**** SLIDER GRID IS PERIODIC IN X ***

set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined 

SERIAL: NOT EXACT **WHY**
$tcmcs sliderRefineGride2.order2 -solver=petsc -predefined  > sr.serial.out

WORKS NOW: 
mpirun -np 1 $tcmcp plug2.hdf -solver=petsc -predefined > plug.np1.out
mpirun -np 1 $tcmcp plug1.hdf -solver=petsc -predefined > plug1.np1.out
SERIAL PLUG -- exact 
$tcmcs plug2.hdf -solver=petsc -predefined  > plug.serial.out
$tcmcs plug1.hdf -solver=petsc -predefined  > plug1.serial.out

mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out



***************************************************************************
****** June 1, 2017

**CHANGES TO USER SUPPLIED EQUATIONS IN PARALLEL***

*NEW* TEST:

  
WORKS: 
mpirun -np 3 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined
mpirun -np 2 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined

mpirun -np 1 valgrind $tcmcp square8.order2.hdf -solver=petsc -predefined

-- looks OK, check valgrind
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out



set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined
$tcmcs square8.order2.hdf -solver=petsc -predefined




***************************************************************************
****** May 23, 2017

ANOTHER BUG -- Shift equationNumber base in PETScSolver to eqnBase=1

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelpNew.out

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=15 > ! matParallelNew.out



***************************************************************************
****** May 21-22, 2017

** ANOTHER BUG FOUND with predefined and user-defined equations

**BUG FOUND WITH PARALLEL and PREDEFINED -- constraint equation bad


CHECK tcmConstraint + predefined:

set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelp.out
$tcmcs square8.order2.hdf -solver=petsc -predefined > ! tcmcSerials.out



! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matParallelp.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matSerialp.out

! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matParallel.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matSerial.out

***************************************************************************
****** May 21, 2017


Tracking down bug with parallel AMP:

alias tcm3p $op/tests/tcm3p
alias tcm3s $op/tests/tcm3s

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matParallel.out

tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matSerial.out


tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -neumann -outputMatrix


***************************************************************************
****** April 1-2, 2017


MORE WORK ON CONSTRAINTS IN PARALLEL PETSC

PRINT OUT SPARSE MATRIX

mpiexec -n 2 xterm -e gdb --args $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12

PARALLEL
mpirun -np 1 $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.parallel.out


SERIAL
$tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.serial.out


***************************************************************************
****** March 13-20, 2017

Implement constraints in parallel petsc

mpiexec -n 2 xterm -e gdb --args $tcmc square8.order2.hdf -solver=petsc


set tcmc = $op/tests/tcmConstraint

$tcmc square8.order2.hdf -solver=petsc

mpirun -np 1 $tcmc square8.order2.hdf -solver=petsc



  


============================
==== Aug 27, 2016

----- TEST NEW opt version of 6th-order Laplacian ----

mpiexec -n 4 tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc -tol=1.e-12
 grid=0 (square) max. rel. err=1.599507e-09 (1.599616e-09 with ghost)

mpiexec -n 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.035096e-07 (1.035096e-07 with ghost)


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined

grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)


--- TEST predefined and 6th order --

mpiexec -n 2 xterm -e gdb --args tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc


mpirun -np 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc



mpirun -np 1 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.123300e-07 (1.123300e-07 with ghost)

============================
==== Aug 18, 2016

--- TEST predefined and 6th order --


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)

tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

tcm3 -g=square128p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)




tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

============================
==== Aug 17, 2016

TEST sixth-order for Jeff

*FIXED* laplacianFDCoefficients>c (old style)

Looks OK -- ratio 64
tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)
G64: grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)
G128:grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)


ORDER=6  -- but results are only 4th-order
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)
G64: grid=0 (square) max. rel. err=1.031297e-06 (1.031297e-06 with ghost)
G128:grid=0 (square) max. rel. err=6.449760e-08 (6.449760e-08 with ghost)


tcm3 -g=square8p.order6.hdf -trig -neumann -order=6



ORDER=4
tcm3 -g=square32p.order4.hdf -trig -neumann -order=4
grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)


-- periodic order 2
tcm3 -g=square32p.order2.hdf -trig -neumann
grid=0 (square) max. rel. err=3.218964e-03 (3.218964e-03 with ghost)


============================
==== July 29, 2016

Test new constraint

tcmConstraint square8.order2.hdf -solver=petsc


tcmConstraint square8.order2.hdf -neumann

****** Oct 11-12, 14 2015.

--- Neumann:

tcmConstraint square8.order2.hdf -neumann

tcmConstraint square8.order2.hdf -neumann -debug=63 >! junk

---- Dirichlet OK: 

tcmConstraint square8.order2.hdf -dirichlet -debug=63 >! junk

tcmConstraint square8.order2.hdf -dirichlet




****** July 1, 2015

mpirun -np 4 tcm3 square32.order2.hdf -solver=petsc -testCommunicator




mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc

PETSC TEST ROUTINE: 
mpiexec -n 2  ex2

 mpiexec -n 2 ex2
Norm of error 0.000591671 iterations 118, m=101, n=101

****** June 30, 2015  --  TEST PETSc communicator

PETSC TEST ROUTINE: 
mpiexec -n 4  ex4

mpiexec -n 2 xterm -e gdb ./tcm3 
run square32.order2.hdf -solver=petsc -testCommunicator


mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc
mpirun -np 1 tcm3 square32.order2.hdf -solver=petsc




*** 2015/05/16 -- more tests on variable coefficients

CIC: 
tcm3 cice2.order2 -mixed

cice4
grid=0 (square) max. rel. err=7.683513e-06 (7.683513e-06 with ghost)
grid=1 (Annulus) max. rel. err=4.251094e-05 (4.251094e-05 with ghost)

cice2:
 grid=0 (square) max. rel. err=4.763309e-05 (4.763309e-05 with ghost)
 grid=1 (Annulus) max. rel. err=2.225289e-04 (2.225289e-04 with ghost)


tcm3 square20pn.order2 -mixed

** square + MIXED BC: exact:
tcm3 square8.order2 -mixed
 grid=0 (square) max. rel. err=6.145234e-16 (9.217852e-16 with ghost)
yale/square8.order2///mixed: error                                : err = 6.15e-16, cpu=1.7e-05(s)  




tcm3 cice2.order2  

*** tcm3 square20.order2


.....solver: size = 2.38e+05 (bytes), grid-pts=625, reals/grid-pt=47.51 
 grid=0 (square) max. rel. err=1.255672e-15 (1.255672e-15 with ghost)
Maximum relative error with dirichlet bc's= 1.255672e-15 (1.255672e-15 with ghost)
yale/square20.order2///dirichlet: error                                                     : err = 1.26e-15, cpu=2.9e-05(s)  
Oges::allocateWorkSpace: numberOfNonzeros=3366 fillinRatio=2.000000e+01
allocateWorkSpace: numberOfEquations=625, nsp = 67320, fillinRatio= 20, numberOfNonzeros = 3366
residual=0.00e+00, time for 1st solve of the Neumann problem = 1.77e-03 (iterations=0)
residual=0.00e+00, time for 2nd solve of the Neumann problem = 3.10e-05 (iterations=0)
 grid=0 (square) max. rel. err=4.101861e-15 (4.185572e-15 with ghost)
Maximum relative error with neumann bc's= 4.101861e-15
yale/square20.order2///neumann: error                                                       : err = 4.10e-15, cpu=3.1e-05(s)  



*** 2014/05/18 -- test variable coefficient BC's

tbcc square16.order2 | grep mixed
square5/square/order=2/std/mixed (var-coeff)          : err = 5.33e-15, cpu=1.5e-04(s)  
cic/square/order=2/std/mixed (var-coeff)              : err = 1.24e-14, cpu=3.8e-04(s)  
cic/Annulus/order=2/std/mixed (var-coeff)             : err = 6.22e-15, cpu=1.8e-04(s)  
sib/box/order=2/std/mixed (var-coeff)                 : err = 1.42e-14, cpu=1.1e-02(s)  
sib/north-pole/order=2/std/mixed (var-coeff)          : err = 4.44e-15, cpu=9.1e-04(s)  
sib/south-pole/order=2/std/mixed (var-coeff)          : err = 4.22e-15, cpu=9.0e-04(s)  

checkop.p tbcc
Running: ./tbcc  > tbcc.out
 compare files tbcc.dp.check.new and tbcc.dp.check 
smartDiff: files tbcc.dp.check.new and tbcc.dp.check agree.
      ...tbcc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================




tbc square16.order2



Regression test for tbc : 
checkop.p tbc 

running tbc...
Running: ./tbc  > tbc.out
 compare files tbc.dp.check.new and tbc.dp.check 
smartDiff: files tbc.dp.check.new and tbc.dp.check agree.
      ...tbc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================